{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6063ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import traceback\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54a0affb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the cleaned .txt files.\n",
    "# Based on your file structure, the correct path is 'output/cleaned_text'.\n",
    "DATA_PATH = \"output/cleaned_text\"\n",
    "\n",
    "# Model for creating text embeddings. This model is efficient and effective.\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# LLM for generating answers. FLAN-T5 is excellent for Q&A tasks.\n",
    "LLM_MODEL = \"google/flan-t5-base\" # Use 'base' for faster performance, can be changed to 'large' for higher accuracy\n",
    "\n",
    "# Path to save/load the vector store to avoid re-creating it every time.\n",
    "DB_FAISS_PATH = \"vectorstore/db_faiss\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65951259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db():\n",
    "    \"\"\"\n",
    "    This function creates a FAISS vector store from the documents\n",
    "    in the DATA_PATH directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"--- Starting: Loading documents ---\")\n",
    "        loader = DirectoryLoader(DATA_PATH, glob=\"*.txt\", show_progress=True)\n",
    "        documents = loader.load()\n",
    "        if not documents:\n",
    "            print(f\"FAILED: No documents found in {DATA_PATH}. Please check the path.\")\n",
    "            return None\n",
    "        print(f\"--- Finished: Loaded {len(documents)} documents ---\")\n",
    "\n",
    "        print(\"--- Starting: Splitting documents into chunks ---\")\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        print(f\"--- Finished: Split into {len(docs)} chunks ---\")\n",
    "\n",
    "        print(\"--- Starting: Creating embeddings (this process can take time) ---\")\n",
    "        embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "        print(\"--- Finished: Embeddings model loaded ---\")\n",
    "\n",
    "        print(\"--- Starting: Creating and saving FAISS vector store ---\")\n",
    "        db = FAISS.from_documents(docs, embeddings)\n",
    "        db.save_local(DB_FAISS_PATH)\n",
    "        print(f\"--- Finished: Vector store created and saved at {DB_FAISS_PATH} ---\")\n",
    "        return db\n",
    "    except Exception as e:\n",
    "        print(\"\\n--- AN ERROR OCCURRED WHILE CREATING THE VECTOR DATABASE ---\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75741d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_qa_chain():\n",
    "    \"\"\"\n",
    "    This function sets up the complete question-answering chain.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(DB_FAISS_PATH):\n",
    "            print(\"--- Loading existing vector store ---\")\n",
    "            embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "            db = FAISS.load_local(DB_FAISS_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "            print(\"--- Vector store loaded successfully ---\")\n",
    "        else:\n",
    "            print(\"--- Vector store not found. Creating a new one. ---\")\n",
    "            db = create_vector_db()\n",
    "            if db is None:\n",
    "                return None\n",
    "\n",
    "        print(\"--- Starting: Setting up LLM (this can take a long time on the first run) ---\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL)\n",
    "\n",
    "        # Hapus parameter 'temperature' dan 'top_p' untuk menghindari warning\n",
    "        pipe = pipeline(\n",
    "            \"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_length=512,\n",
    "            repetition_penalty=1.2\n",
    "        )\n",
    "\n",
    "        llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        print(\"--- Finished: LLM setup complete ---\")\n",
    "\n",
    "        # Buat Prompt Template untuk memberikan instruksi yang lebih baik\n",
    "        prompt_template = \"\"\"\n",
    "Gunakan potongan konteks berikut untuk menjawab pertanyaan di akhir. Jawablah hanya berdasarkan konteks yang diberikan. Jika Anda tidak tahu jawabannya dari konteks, katakan saja bahwa Anda tidak dapat menemukan jawaban dalam dokumen yang disediakan. Jangan mencoba mengarang jawaban.\n",
    "\n",
    "Konteks:\n",
    "{context}\n",
    "\n",
    "Pertanyaan: {question}\n",
    "Jawaban:\n",
    "\"\"\"\n",
    "        PROMPT = PromptTemplate(\n",
    "            template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "        \n",
    "        chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "\n",
    "        retriever = db.as_retriever()\n",
    "\n",
    "        # Masukkan prompt kustom ke dalam QA Chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            chain_type_kwargs=chain_type_kwargs,\n",
    "            return_source_documents=True\n",
    "        )\n",
    "        \n",
    "        return qa_chain\n",
    "    except Exception as e:\n",
    "        print(\"\\n--- AN ERROR OCCURRED WHILE SETTING UP THE QA CHAIN ---\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c878ab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading existing vector store ---\n",
      "--- Vector store loaded successfully ---\n",
      "--- Starting: Setting up LLM (this can take a long time on the first run) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished: LLM setup complete ---\n",
      "\n",
      "==================================================\n",
      "AI Question Answering System is Ready!\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_chain = setup_qa_chain()\n",
    "\n",
    "if qa_chain:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"AI Question Answering System is Ready!\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "else:\n",
    "    print(\"\\nFAILED: The system could not be started. Please check the error messages above.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b24b491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... AI is processing your question ...\n",
      "\n",
      "--------------------\n",
      "Question: What is islam about?\n",
      "\n",
      "AI Answer:\n",
      "[\n",
      "\n",
      "(Answer generated in 65.13 seconds)\n",
      "\n",
      "--- Source Documents Used ---\n",
      "-> File: output\\cleaned_text\\معالم السنة النبوية -.txt\n"
     ]
    }
   ],
   "source": [
    "# Change the query below\n",
    "query = \"\"\n",
    "\n",
    "if qa_chain:\n",
    "    try:\n",
    "        print(\"... AI is processing your question ...\")\n",
    "        start_time = time.time()\n",
    "        result = qa_chain({\"query\": query})\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*20)\n",
    "        print(\"Question:\", query)\n",
    "        print(\"\\nAI Answer:\")\n",
    "        # Check if the result exists and is not empty\n",
    "        if result and result.get(\"result\"):\n",
    "            print(result[\"result\"].strip()) # Gunakan .strip() untuk menghapus spasi kosong\n",
    "        else:\n",
    "            print(\"The AI could not generate an answer. The received result was empty.\")\n",
    "            print(\"Raw result:\", result)\n",
    "\n",
    "        print(f\"\\n(Answer generated in {end_time - start_time:.2f} seconds)\")\n",
    "        \n",
    "        print(\"\\n--- Source Documents Used ---\")\n",
    "        # Use a set to ensure unique sources\n",
    "        source_files = set()\n",
    "        if result and result.get(\"source_documents\"):\n",
    "            for doc in result[\"source_documents\"]:\n",
    "                source_files.add(doc.metadata.get('source', 'N/A'))\n",
    "        \n",
    "        for file_path in source_files:\n",
    "            print(f\"-> File: {file_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n--- AN ERROR OCCURRED WHILE ANSWERING THE QUESTION ---\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"Error: The QA chain was not initialized. Cannot run the Q&A session.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
